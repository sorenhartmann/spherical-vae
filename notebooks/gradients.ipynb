{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import SphericalVAE\n",
    "from scipy.special import ive\n",
    "from torch.distributions.kl import kl_divergence\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.double)\n",
    "svae = SphericalVAE(3, 2, encoder_params={\"layer_sizes\" : [5]}, decoder_params={\"layer_sizes\" : [5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = svae(x)\n",
    "px, pz, qz, z = [output[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
    "\n",
    "kl_term = kl_divergence(qz, pz)\n",
    "log_px = px.log_prob(x).sum(-1)\n",
    "\n",
    "loss = -log_px + kl_term\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we try to retrieve the loss wrt. $k$, $\\mu$ and the parameters of the decoder, and then use these to figure out the gradient wrt. the parameters of the model as a whole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.0227, dtype=torch.float64)"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_d_k.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "svae.zero_grad()\n",
    "\n",
    "log_px_d_k, = torch.autograd.grad(log_px, qz.k, grad_outputs=torch.ones_like(qz.k),  retain_graph=True)\n",
    "kl_term_d_k, = torch.autograd.grad(kl_term, qz.k, grad_outputs=torch.ones_like(qz.k),  retain_graph=True)\n",
    "\n",
    "loss_d_k = (-log_px_d_k + kl_term_d_k) / len(qz.k)\n",
    "\n",
    "loss_d_mu = torch.autograd.grad(loss, qz.mu, retain_graph=True)\n",
    "loss_d_decoder = torch.autograd.grad(loss, svae.decoder.parameters(), retain_graph=True)\n",
    "\n",
    "torch.autograd.backward(svae.decoder.parameters(), grad_tensors=loss_d_decoder, retain_graph=True)\n",
    "torch.autograd.backward(qz.k, grad_tensors=loss_d_k, retain_graph=True)\n",
    "torch.autograd.backward(qz.mu, grad_tensors=loss_d_mu, retain_graph=True)\n",
    "\n",
    "decomposed = []\n",
    "for param in svae.parameters():\n",
    "    decomposed.append(param.grad.clone())\n",
    "\n",
    "\n",
    "svae.zero_grad()\n",
    "loss.mean().backward(retain_graph=True)\n",
    "\n",
    "usual = []\n",
    "for param in svae.parameters():\n",
    "    usual.append(param.grad.clone())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(torch.isclose(a,b).all() for a, b in zip(decomposed, usual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can add the correspond gradient wrt. $k$, again using autograd to calculate the gradient of the correction term wrt. kappa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "svae.zero_grad()\n",
    "\n",
    "log_px_d_k, = torch.autograd.grad(log_px, qz.k, grad_outputs=torch.ones_like(qz.k),  retain_graph=True)\n",
    "kl_term_d_k, = torch.autograd.grad(kl_term, qz.k, grad_outputs=torch.ones_like(qz.k),  retain_graph=True)\n",
    "\n",
    "loss_d_mu, = torch.autograd.grad(loss, qz.mu, retain_graph=True)\n",
    "loss_d_decoder = torch.autograd.grad(loss, svae.decoder.parameters(), retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.5009, 0.6706], dtype=torch.float64)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = qz.saved_for_grad[\"eps\"]\n",
    "w = qz.saved_for_grad[\"w\"]\n",
    "b = qz.saved_for_grad[\"b\"]\n",
    "\n",
    "corr_term = (\n",
    "    w * qz.k\n",
    "    + 1 / 2 * (qz.m - 3) * torch.log(1 - w ** 2)\n",
    "    + torch.log(torch.abs(((-2 * b) / (((b - 1) * eps + 1) ** 2))))\n",
    ")\n",
    "\n",
    "corr_term_d_k, = torch.autograd.grad(corr_term, qz.k, grad_outputs=torch.ones_like(corr_term), retain_graph=True)\n",
    "corr_term_d_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can construct the corrected gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.1673, -4.4906], dtype=torch.float64)"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    g_cor = log_px * ( -ive(qz.m/2, qz.k)/ive(qz.m/2-1, qz.k) + corr_term_d_k)\n",
    "g_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_px_d_k_adj = log_px_d_k + g_cor\n",
    "loss_d_k = (-log_px_d_k_adj + kl_term_d_k) / len(qz.k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.backward(svae.decoder.parameters(), grad_tensors=loss_d_decoder, retain_graph=True)\n",
    "torch.autograd.backward(qz.k, grad_tensors=loss_d_k, retain_graph=True)\n",
    "torch.autograd.backward(qz.mu, grad_tensors=loss_d_mu, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted = []\n",
    "for param in svae.parameters():\n",
    "    adjusted.append(param.grad.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without correction\n",
      "[[ 0.          0.          0.        ]\n",
      " [-3.84098219 -4.80122774 -5.76147329]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "With correction\n",
      "[[ 0.          0.          0.        ]\n",
      " [-1.0841667  -1.35520837 -1.62625005]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "Without correction\n",
      "[ 0.         -0.96024555  0.          0.          0.        ]\n",
      "With correction\n",
      "[ 0.         -0.27104167  0.          0.          0.        ]\n",
      "Without correction\n",
      "[[ 0.          4.6254999   0.          0.          0.        ]\n",
      " [ 0.         -9.42140313  0.          0.          0.        ]\n",
      " [ 0.          1.33468114  0.          0.          0.        ]]\n",
      "With correction\n",
      "[[ 0.          4.6254999   0.          0.          0.        ]\n",
      " [ 0.         -9.42140313  0.          0.          0.        ]\n",
      " [ 0.          2.98612796  0.          0.          0.        ]]\n",
      "Without correction\n",
      "[  8.65865103 -19.69938238   2.81064806]\n",
      "With correction\n",
      "[  8.65865103 -19.69938238   5.72904697]\n",
      "Without correction\n",
      "[[ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.91988794 10.347274  ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.1269169   1.87964072]]\n",
      "With correction\n",
      "[[ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.91988794 10.347274  ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.1269169   1.87964072]]\n",
      "Without correction\n",
      "[ 0.          0.         10.42292528  0.          1.8933552 ]\n",
      "With correction\n",
      "[ 0.          0.         10.42292528  0.          1.8933552 ]\n",
      "Without correction\n",
      "[[ 0.          0.         -0.36753414  0.         -0.29090781]\n",
      " [ 0.          0.         -0.30092884  0.         -0.24037946]\n",
      " [ 0.          0.         -0.62822151  0.         -0.50549815]\n",
      " [ 0.          0.         -1.25882024  0.         -0.95818144]\n",
      " [ 0.          0.         -1.14084113  0.         -0.87440987]\n",
      " [ 0.          0.         -3.1279685   0.         -2.43924324]]\n",
      "With correction\n",
      "[[ 0.          0.         -0.36753414  0.         -0.29090781]\n",
      " [ 0.          0.         -0.30092884  0.         -0.24037946]\n",
      " [ 0.          0.         -0.62822151  0.         -0.50549815]\n",
      " [ 0.          0.         -1.25882024  0.         -0.95818144]\n",
      " [ 0.          0.         -1.14084113  0.         -0.87440987]\n",
      " [ 0.          0.         -3.1279685   0.         -2.43924324]]\n",
      "Without correction\n",
      "[ -2.17567801  -1.83633253  -3.92583096  -6.49404738  -6.03666078\n",
      " -17.599096  ]\n",
      "With correction\n",
      "[ -2.17567801  -1.83633253  -3.92583096  -6.49404738  -6.03666078\n",
      " -17.599096  ]\n"
     ]
    }
   ],
   "source": [
    "for a, b in zip(usual, adjusted):\n",
    "    print(\"Without correction\")\n",
    "    print(a.numpy())\n",
    "    print(\"With correction\")\n",
    "    print(b.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('spherical-vae': conda)",
   "name": "python377jvsc74a57bd0655ab5e98cfd1c434f76cb171fbd09e46bd40e25e7cc42505bd9bc3a9c4b997b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}